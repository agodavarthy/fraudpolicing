{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agodavarthy/fraudpolicing/blob/main/Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_dj0FAY3nwVP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import math\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import sys\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "file_path = \"/content/drive/MyDrive/IK_ProjectUp/FraudDetection/May2/data/\"\n",
        "\n",
        "#train_df = pd.read_csv(file_path+\"smaller_fraud.csv\", index_col=0)\n",
        "train_df = pd.read_csv(file_path+\"fraudTrain.csv\", index_col=0)\n",
        "test_df = pd.read_csv(file_path+\"fraudTest.csv\", index_col=0)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5HWwFpgFe9p",
        "outputId": "cdcadb8d-7b25-43ba-f95c-e6095dc5147a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "err75JpdqdSE",
        "outputId": "a519f084-e534-4287-9e9b-a64602d8838a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1296675, 22)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkaAjNA_qfBy",
        "outputId": "111250b3-ad11-45b3-ee6a-d5f9804de2e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(555719, 22)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([train_df, test_df], axis=0)"
      ],
      "metadata": {
        "id": "n-HQr4mAp3ZS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmWWX3vXqrV4",
        "outputId": "f32123b0-2866-41a0-ffaa-8d401a8e31bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1852394, 22)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_datetime(df):\n",
        "    if 'trans_date_trans_time' in df.columns and 'dob' in df.columns:\n",
        "      df['trans_dt'] = pd.to_datetime(df['trans_date_trans_time'])\n",
        "      df['trans_year'] = df['trans_dt'].dt.year\n",
        "      df['trans_month'] = df['trans_dt'].dt.month\n",
        "      df['trans_day'] = df['trans_dt'].dt.day\n",
        "      df['trans_dayofweek'] = df['trans_dt'].dt.dayofweek\n",
        "      df['trans_hour'] = df['trans_dt'].dt.hour\n",
        "      df['dob_dt'] = pd.to_datetime(df['dob'])\n",
        "    return df"
      ],
      "metadata": {
        "id": "aoe1wRn1pX_K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_transaction_freq(df, timestamp_col, entity_col, window='1H'):\n",
        "    \"\"\"\n",
        "    Computes the transaction rate for each entity within a specified time window.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame with transaction data.\n",
        "        timestamp_col (str): The name of the timestamp column.\n",
        "        entity_col (str): The name of the column that identifies the entity.\n",
        "        window (str): The rolling time window for calculating the rate.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The input DataFrame with an added transaction rate column.\n",
        "    \"\"\"\n",
        "    # Create a working copy\n",
        "    result_df = df.copy()\n",
        "\n",
        "    # Ensure timestamp is datetime type\n",
        "    result_df[timestamp_col] = pd.to_datetime(result_df[timestamp_col])\n",
        "\n",
        "    # Sort by entity and timestamp (critical for rolling window operations)\n",
        "    result_df = result_df.sort_values(by=[entity_col, timestamp_col])\n",
        "\n",
        "    # Name of the new rate column\n",
        "    rate_col_name = f'transaction_rate_{window}'\n",
        "\n",
        "    # Create a unique row identifier to ensure perfect alignment later\n",
        "    result_df['_temp_row_id'] = range(len(result_df))\n",
        "\n",
        "    # APPROACH: Process each entity group separately with explicit time-based calculations\n",
        "    all_results = []\n",
        "\n",
        "    for entity, group in result_df.groupby(entity_col):\n",
        "        #print(\"Hey Jude\")\n",
        "        # Create a copy of this entity's data\n",
        "        entity_df = group.copy()\n",
        "        #print(\"grouped entity = \")\n",
        "        #print(entity_df[['cc_num', 'trans_dt']])\n",
        "        # Create a dummy column for counting\n",
        "        entity_df['_count'] = 1\n",
        "        #print(\"grouped entity(after count) = \")\n",
        "        #print(entity_df[['cc_num', 'trans_dt', '_count']])\n",
        "        #print(\"------------------\")\n",
        "        #sys.exit(1)\n",
        "        # Set timestamp as index for time-based operations\n",
        "        entity_df.set_index(timestamp_col, inplace=True)\n",
        "\n",
        "        # Calculate rolling count within the window\n",
        "        # For each row, this counts how many transactions happened in the preceding window\n",
        "        entity_df[rate_col_name] = entity_df['_count'].rolling(window=window, closed='left').sum().fillna(0)\n",
        "\n",
        "        # Reset index to get timestamp back as a column\n",
        "        entity_df.reset_index(inplace=True)\n",
        "\n",
        "        # Keep this group's results\n",
        "        all_results.append(entity_df)\n",
        "\n",
        "    # Combine all entity results\n",
        "    combined_results = pd.concat(all_results, ignore_index=False)\n",
        "    #print(\"Combined results = \", combined_results)\n",
        "    # Sort by our temporary row ID to restore original order\n",
        "    combined_results = combined_results.sort_values('_temp_row_id')\n",
        "\n",
        "    # Clean up temporary columns\n",
        "    combined_results.drop(columns=['_count', '_temp_row_id'], inplace=True)\n",
        "\n",
        "    return combined_results"
      ],
      "metadata": {
        "id": "lYIZlCYl5MMj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_avg_transaction_amount(df, timestamp_col, entity_col, amount_col, window='1H'):\n",
        "    \"\"\"\n",
        "    Computes the average transaction amount for each entity within a specified time window.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame with transaction data.\n",
        "        timestamp_col (str): The name of the timestamp column.\n",
        "        entity_col (str): The name of the column that identifies the entity.\n",
        "        amount_col (str): The name of the column containing transaction amounts.\n",
        "        window (str): The rolling time window for calculating the average.\n",
        "                      Examples: '1H' (1 hour), '1D' (1 day), '7D' (7 days), '30min'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The input DataFrame with an added column representing the\n",
        "                      average transaction amount per the specified time window.\n",
        "                      The new column will be named 'avg_amount_' + window.\n",
        "    \"\"\"\n",
        "    # Create a working copy\n",
        "    result_df = df.copy()\n",
        "\n",
        "    # Ensure timestamp is datetime type\n",
        "    result_df[timestamp_col] = pd.to_datetime(result_df[timestamp_col])\n",
        "\n",
        "    # Sort by entity and timestamp (critical for rolling window operations)\n",
        "    result_df = result_df.sort_values(by=[entity_col, timestamp_col])\n",
        "\n",
        "    # Name of the new rate column\n",
        "    avg_col_name = f'avg_amount_{window}'\n",
        "\n",
        "    # Create a unique row identifier to ensure perfect alignment later\n",
        "    result_df['_temp_row_id'] = range(len(result_df))\n",
        "\n",
        "    # Process each entity group separately\n",
        "    all_results = []\n",
        "\n",
        "    for entity, group in result_df.groupby(entity_col):\n",
        "        # Create a copy of this entity's data\n",
        "        entity_df = group.copy()\n",
        "\n",
        "        # Set timestamp as index for time-based operations\n",
        "        entity_df.set_index(timestamp_col, inplace=True)\n",
        "        #print(\"entity_df[amount_col] = \", entity_df[[entity_col, amount_col]])\n",
        "        \"\"\"\n",
        "        # Calculate rolling sum of amounts within the window\n",
        "        rolling_sum = entity_df[amount_col].rolling(window=window, closed='left').sum()\n",
        "\n",
        "        # Calculate rolling count of transactions within the window\n",
        "        rolling_count = entity_df[amount_col].rolling(window=window, closed='left').count()\n",
        "\n",
        "        # Calculate average amount (handling division by zero)\n",
        "        # When count is 0, we'll get NaN which we fill with 0 (no transactions means avg amount is 0)\n",
        "        entity_df[avg_col_name] = (rolling_sum / rolling_count).fillna(0)\n",
        "        \"\"\"\n",
        "\n",
        "        rolling_mean = entity_df[amount_col].rolling(window=window, closed='left').mean().fillna(0)\n",
        "        entity_df[avg_col_name] = rolling_mean\n",
        "        # Reset index to get timestamp back as a column\n",
        "        entity_df.reset_index(inplace=True)\n",
        "\n",
        "        # Keep this group's results\n",
        "        all_results.append(entity_df)\n",
        "\n",
        "    # Combine all entity results\n",
        "    combined_results = pd.concat(all_results, ignore_index=False)\n",
        "\n",
        "    # Sort by our temporary row ID to restore original order\n",
        "    combined_results = combined_results.sort_values('_temp_row_id')\n",
        "\n",
        "    # Clean up temporary column\n",
        "    combined_results.drop(columns=['_temp_row_id'], inplace=True)\n",
        "\n",
        "    return combined_results"
      ],
      "metadata": {
        "id": "F4nDLED89aem"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_avg_transaction_interval(df, timestamp_col, entity_col, window='30D'):\n",
        "    \"\"\"\n",
        "    Computes the average time interval between transactions for each entity within a specified time window.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame with transaction data.\n",
        "        timestamp_col (str): The name of the timestamp column.\n",
        "        entity_col (str): The name of the column that identifies the entity (e.g., cc_num).\n",
        "        window (str): The rolling time window for calculating the average interval.\n",
        "                      Examples: '30D' (30 days), '1M' (1 month), '90D' (90 days).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The input DataFrame with an added column representing the\n",
        "                      average transaction interval in hours within the specified time window.\n",
        "                      The new column will be named 'avg_interval_hours_' + window.\n",
        "    \"\"\"\n",
        "    # Create a working copy\n",
        "    result_df = df.copy()\n",
        "\n",
        "    # Ensure timestamp is datetime type\n",
        "    result_df[timestamp_col] = pd.to_datetime(result_df[timestamp_col])\n",
        "\n",
        "    # Sort by entity and timestamp (critical for calculating intervals)\n",
        "    result_df = result_df.sort_values(by=[entity_col, timestamp_col])\n",
        "\n",
        "    # Name of the new column\n",
        "    interval_col_name = f'avg_interval_hours_{window}'\n",
        "\n",
        "    # Create a unique row identifier to ensure perfect alignment later\n",
        "    result_df['_temp_row_id'] = range(len(result_df))\n",
        "\n",
        "    # Process each entity group separately\n",
        "    all_results = []\n",
        "\n",
        "    for entity, group in result_df.groupby(entity_col):\n",
        "        # Create a copy of this entity's data\n",
        "        entity_df = group.copy()\n",
        "\n",
        "        # Calculate the time difference between consecutive transactions for this entity\n",
        "        entity_df['_time_diff'] = entity_df[timestamp_col].diff()\n",
        "\n",
        "        # Convert time differences to hours\n",
        "        entity_df['_interval_hours'] = entity_df['_time_diff'].dt.total_seconds() / 3600\n",
        "\n",
        "        # Set timestamp as index for time-based operations\n",
        "        entity_df.set_index(timestamp_col, inplace=True)\n",
        "\n",
        "        # For each transaction, calculate the average interval over the past window\n",
        "        # We'll use a rolling window to look back at previous intervals\n",
        "\n",
        "        # First, handle the first transaction for each entity (no interval)\n",
        "        entity_df.loc[entity_df['_interval_hours'].isna(), '_interval_hours'] = 0\n",
        "\n",
        "        # Calculate rolling mean of intervals within the specified window\n",
        "        entity_df[interval_col_name] = entity_df['_interval_hours'].rolling(window=window, closed='left').mean().fillna(0)\n",
        "\n",
        "        # Reset index to get timestamp back as a column\n",
        "        entity_df.reset_index(inplace=True)\n",
        "\n",
        "        # Keep this group's results\n",
        "        all_results.append(entity_df)\n",
        "\n",
        "    # Combine all entity results\n",
        "    combined_results = pd.concat(all_results, ignore_index=False)\n",
        "\n",
        "    # Sort by our temporary row ID to restore original order\n",
        "    combined_results = combined_results.sort_values('_temp_row_id')\n",
        "\n",
        "    # Clean up temporary columns\n",
        "    combined_results.drop(columns=['_temp_row_id', '_time_diff', '_interval_hours'], inplace=True)\n",
        "\n",
        "    return combined_results"
      ],
      "metadata": {
        "id": "xzDqvcYizKwJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "BNG7AN6T1Acv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_age(df, curr_dt, dob_dt):\n",
        "  df['cust_age'] = ((df[curr_dt] - df[dob_dt]).dt.days //365.25).astype(int)\n",
        "  return df"
      ],
      "metadata": {
        "id": "EJdJRAYl1QH3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getModel(model_nm):\n",
        "  if model_nm == \"logistic\":\n",
        "    model = LogisticRegression(max_iter=200, random_state=42)\n",
        "    return model\n",
        "  elif model_nm == \"xgb\":\n",
        "    model = XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False)\n",
        "    \"\"\"\n",
        "    model = XGBClassifier(\n",
        "      objective='binary:logistic',\n",
        "      num_class=\"2\",\n",
        "      learning_rate=0.01,\n",
        "      n_estimators=3\n",
        "    )\n",
        "    \"\"\"\n",
        "    return model\n",
        "  else:\n",
        "    print(\"Sorry....\")\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "LiVtnC-91R4a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to calculate the Haversine distance between two points\n",
        "def calculate_distance(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"\n",
        "    Calculate the Haversine distance in kilometers between two points\n",
        "    specified by latitude and longitude.\n",
        "\n",
        "    Args:\n",
        "      lat1, lon1: Latitude and longitude of the first point.\n",
        "      lat2, lon2: Latitude and longitude of the second point.\n",
        "\n",
        "    Returns:\n",
        "      Distance in kilometers.\n",
        "    \"\"\"\n",
        "    # Earth radius in kilometers\n",
        "    R = 6371\n",
        "\n",
        "    # Convert latitude and longitude from degrees to radians\n",
        "    phi1 = math.radians(lat1)\n",
        "    phi2 = math.radians(lat2)\n",
        "    delta_phi = math.radians(lat2 - lat1)\n",
        "    delta_lambda = math.radians(lon2 - lon1)\n",
        "\n",
        "    # Apply the Haversine formula\n",
        "    a = math.sin(delta_phi / 2.0)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2.0)**2\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "    distance = R * c\n",
        "    return distance"
      ],
      "metadata": {
        "id": "xJFU2a-NfcgI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_transformation(data):\n",
        "\n",
        "  df = data.copy()\n",
        "\n",
        "  df = transform_datetime(df)\n",
        "  df = compute_transaction_freq(df, \"trans_dt\", \"cc_num\", window='30D')\n",
        "\n",
        "  df = compute_avg_transaction_amount(df, \"trans_dt\", \"cc_num\", 'amt', window='30D')\n",
        "\n",
        "  df = compute_avg_transaction_interval(df, \"trans_dt\", \"cc_num\", window='30D')\n",
        "  df = compute_age(df, 'trans_dt', 'dob_dt')\n",
        "\n",
        "  df['distance'] = df.apply(lambda row: calculate_distance(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
        "\n",
        "  print(\"Data Columns -----\")\n",
        "  print(df.columns)\n",
        "\n",
        "  label_encoder = LabelEncoder()\n",
        "\n",
        "  categorical_cols = ['merchant', 'first', 'last', 'gender', 'street', 'city', 'state', 'job']\n",
        "  label_encoders = [label_encoder.fit(df[col]) for col in categorical_cols]\n",
        "\n",
        "  for col in categorical_cols:\n",
        "    df[col] = label_encoder.fit_transform(df[col].astype(str))\n",
        "\n",
        "  all_categories = pd.unique(pd.concat([df['category']]))\n",
        "  category_to_id = {category: i + 1 for i, category in enumerate(all_categories)}\n",
        "  df['category'] = df['category'].map(category_to_id)\n",
        "\n",
        "  print(\"Data transformation complete\")\n",
        "  return df"
      ],
      "metadata": {
        "id": "VDHCLQNjWmTy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_categorical_data(df):\n",
        "\n",
        "  label_encoder = LabelEncoder()\n",
        "\n",
        "  categorical_cols = ['merchant', 'first', 'last', 'gender', 'street', 'city', 'state', 'job']\n",
        "  label_encoders = [label_encoder.fit(df[col]) for col in categorical_cols]\n",
        "\n",
        "  for col in categorical_cols:\n",
        "    df[col] = label_encoder.fit_transform(df[col].astype(str))\n",
        "\n",
        "  all_categories = pd.unique(pd.concat([df['category']]))\n",
        "  category_to_id = {category: i + 1 for i, category in enumerate(all_categories)}\n",
        "  df['category'] = df['category'].map(category_to_id)\n",
        "\n",
        "  print(\"Data transformation complete\")\n",
        "  return df"
      ],
      "metadata": {
        "id": "k3jQhLZtk-M9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(df, split_date):\n",
        "  df = df.sort_values(by=['trans_date_trans_time','cc_num'])\n",
        "\n",
        "  df = df.dropna()\n",
        "\n",
        "  df_train = df[df['trans_date_trans_time'] < split_date]\n",
        "  df_test = df[df['trans_date_trans_time'] >= split_date]\n",
        "\n",
        "\n",
        "  y_train = df_train['is_fraud']\n",
        "  x_train = df_train.drop(columns=\"is_fraud\")\n",
        "\n",
        "  y_test = df_test['is_fraud']\n",
        "  x_test = df_test.drop(columns=\"is_fraud\")\n",
        "\n",
        "  x_train = x_train.drop([\"dob\", \"cc_num\", \"dob_dt\", \"trans_num\", \"trans_dt\", \"trans_date_trans_time\"], axis=1)\n",
        "  x_test = x_test.drop([\"dob\", \"cc_num\", \"dob_dt\",\"trans_num\", \"trans_dt\", \"trans_date_trans_time\"], axis=1)\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ],
      "metadata": {
        "id": "1ywSkeyDsu3f"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_numeric_data(x_train, x_test, scaler_type):\n",
        "    numerical_features = x_train.select_dtypes(include=['number']).columns.tolist()\n",
        "      # Initialize the scaler\n",
        "    if scaler_type == 'standard':\n",
        "        scaler = StandardScaler()\n",
        "    elif scaler_type == 'minmax':\n",
        "        scaler = MinMaxScaler()\n",
        "    elif scaler_type == 'robust':\n",
        "        scaler = RobustScaler()\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid scaler_type: {scaler_type}.  Must be 'standard', 'minmax', or 'robust'.\")\n",
        "    # Apply scaling to the numerical features in the training set\n",
        "    x_train_scaled = x_train.copy() # Create a copy to avoid modifying the original DataFrame\n",
        "    x_train_scaled[numerical_features] = scaler.fit_transform(x_train[numerical_features])\n",
        "\n",
        "    # Apply the same scaling to the numerical features in the testing set\n",
        "    x_test_scaled = x_test.copy() # Create a copy\n",
        "    x_test_scaled[numerical_features] = scaler.transform(x_test[numerical_features])\n",
        "\n",
        "    return x_train_scaled, x_test_scaled, numerical_features, scaler"
      ],
      "metadata": {
        "id": "o6bvtUbnvyi8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_phase(model, x_train, y_train):\n",
        "  #logreg = LogisticRegression(random_state=42)\n",
        "  print(\"Starting training...\")\n",
        "  print(\"Shape of x_train:\", x_train.shape)\n",
        "\n",
        "\n",
        "  print(\"Shape of y_train:\", y_train.shape)\n",
        "  print(\"Shape of y_train:\", y_train.dtype)\n",
        "\n",
        "  print(\"Shape of y_test:\", y_test.shape)\n",
        "  print(\"Shape of y_test:\", y_test.dtype)\n",
        "\n",
        "  model.fit(x_train, y_train)\n",
        "  print(\"Training complete\")\n",
        "  return model"
      ],
      "metadata": {
        "id": "aBvour7OxW-n"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testing_phase(model, x_test, y_test):\n",
        "  y_pred = model.predict(x_test)\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "06J2MryPxYmf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(y_pred, y_test):\n",
        "  print(\"got predictions:\", y_pred)\n",
        "  print(\"True predictions:\", y_test)\n",
        "  # Evaluate the model\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred)\n",
        "  recall = recall_score(y_test, y_pred)\n",
        "  f1 = f1_score(y_test, y_pred)\n",
        "  report = classification_report(y_test, y_pred)\n",
        "\n",
        "  return accuracy, precision, recall, f1, report"
      ],
      "metadata": {
        "id": "jechLzv9F1ZQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline performace"
      ],
      "metadata": {
        "id": "PX3k1blHTWw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBVaPf0PWbvB",
        "outputId": "08f849de-f748-4539-968a-d4022836bd3b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['trans_date_trans_time', 'cc_num', 'merchant', 'category', 'amt',\n",
              "       'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat',\n",
              "       'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', 'merch_lat',\n",
              "       'merch_long', 'is_fraud'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['distance'], axis=1)"
      ],
      "metadata": {
        "id": "4bmNZ_lG6a1S"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yWWj-eT5zWc",
        "outputId": "8d8b2c23-dd93-467a-91a7-99ae2428bea1"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['trans_dt', 'trans_date_trans_time', 'cc_num', 'merchant', 'category',\n",
              "       'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip',\n",
              "       'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time',\n",
              "       'merch_lat', 'merch_long', 'is_fraud', 'trans_year', 'trans_month',\n",
              "       'trans_day', 'trans_dayofweek', 'trans_hour', 'dob_dt', 'distance'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = data_transformation(df)\n",
        "df.columns\n",
        "df = handle_categorical_data(df)\n",
        "split_date = \"2020-06-01 00:00:01\"\n",
        "x_train, y_train, x_test, y_test = split_data(df, split_date)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv-xXfiGwVWq",
        "outputId": "d38fe6d5-93bd-41ed-95b9-4ab578658282"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Columns -----\n",
            "Index(['trans_dt', 'trans_date_trans_time', 'cc_num', 'merchant', 'category',\n",
            "       'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip',\n",
            "       'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time',\n",
            "       'merch_lat', 'merch_long', 'is_fraud', 'trans_year', 'trans_month',\n",
            "       'trans_day', 'trans_dayofweek', 'trans_hour', 'dob_dt',\n",
            "       'transaction_rate_30D', 'avg_amount_30D', 'avg_interval_hours_30D',\n",
            "       'cust_age', 'distance'],\n",
            "      dtype='object')\n",
            "Data transformation complete\n",
            "Data transformation complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_type = \"standard\"\n",
        "x_train_scaled, x_test_scaled, numerical_features, scaler = normalize_numeric_data(x_train, x_test, scaler_type)"
      ],
      "metadata": {
        "id": "jqActegoye6u"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = getModel('xgb')\n",
        "model = training_phase(model, x_train_scaled, y_train)\n",
        "y_pred = testing_phase(model, x_test_scaled, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weVsfb_jyk7n",
        "outputId": "608b42a1-257c-4b89-96bd-6f161543a601"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Shape of x_train: (1238928, 27)\n",
            "Shape of y_train: (1238928,)\n",
            "Shape of y_train: int64\n",
            "Shape of y_test: (613466,)\n",
            "Shape of y_test: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [19:15:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf5sO9K1EnGk",
        "outputId": "fd0059b0-cf48-437c-9194-d60c51fbcb78"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['trans_dt', 'trans_date_trans_time', 'cc_num', 'merchant', 'category',\n",
              "       'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip',\n",
              "       'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time',\n",
              "       'merch_lat', 'merch_long', 'is_fraud', 'trans_year', 'trans_month',\n",
              "       'trans_day', 'trans_dayofweek', 'trans_hour', 'dob_dt',\n",
              "       'transaction_rate_30D', 'avg_amount_30D', 'avg_interval_hours_30D',\n",
              "       'cust_age', 'distance'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, precision, recall, f1, report = evaluation(y_pred, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGGz_4N_NzX5",
        "outputId": "7bf68365-763c-4709-d483-ce00ff8d16c9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "got predictions: [0 0 0 ... 0 0 0]\n",
            "True predictions: 2438    0\n",
            "511     0\n",
            "964     0\n",
            "1001    0\n",
            "997     0\n",
            "       ..\n",
            "2194    0\n",
            "3660    0\n",
            "3650    0\n",
            "2920    0\n",
            "3650    0\n",
            "Name: is_fraud, Length: 613466, dtype: int64\n",
            "Accuracy: 0.9987790684406308\n",
            "Precision: 0.9635584137191854\n",
            "Recall: 0.7252924566357403\n",
            "F1 Score: 0.8276179516685845\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    610987\n",
            "           1       0.96      0.73      0.83      2479\n",
            "\n",
            "    accuracy                           1.00    613466\n",
            "   macro avg       0.98      0.86      0.91    613466\n",
            "weighted avg       1.00      1.00      1.00    613466\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8-O00goVaE-",
        "outputId": "b48d941a-f88b-424b-c86d-b9b557ccfaaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 22)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New feature: compute_transaction_rate"
      ],
      "metadata": {
        "id": "40q4oFSrTqml"
      }
    },
    {
      "source": [
        "def compute_transaction_rate(df, timestamp_col, entity_col, window='1H'):\n",
        "    \"\"\"\n",
        "    Computes the transaction rate for each entity within a specified time window.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame with transaction data.\n",
        "        timestamp_col (str): The name of the timestamp column.\n",
        "        entity_col (str): The name of the column that identifies the entity\n",
        "                          (e.g., user_id, account_id, card_id).\n",
        "        window (str): The rolling time window for calculating the rate.\n",
        "                      Examples: '1H' (1 hour), '1D' (1 day), '7D' (7 days), '30min'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The input DataFrame with an added column representing the\n",
        "                      transaction rate per the specified window. The new column\n",
        "                      will be named 'transaction_rate_' + window.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure the timestamp column is in datetime format\n",
        "    df[timestamp_col] = pd.to_datetime(df[timestamp_col])  # Convert to datetime\n",
        "    # Sort the DataFrame by the entity and then by timestamp\n",
        "    df = df.sort_values(by=[entity_col, timestamp_col])\n",
        "    # Calculate the rolling count of transactions within the specified time window\n",
        "    rate_col_name = f'transaction_rate_{window}'\n",
        "    # Apply rolling window and calculate mean within each group\n",
        "    df[rate_col_name] = (\n",
        "        df.groupby(entity_col)['amt']\n",
        "        .rolling(window=window, closed='left')\n",
        "        .mean()\n",
        "        .reset_index(level=0, drop=True)  # Remove the group index\n",
        "    )\n",
        "\n",
        "    return df"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2sA_82CQV-jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_train_df = compute_transaction_rate(train_df, 'trans_date_trans_time', 'cc_num', window=3)"
      ],
      "metadata": {
        "id": "lyMX7a3mUrCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run pipeline again"
      ],
      "metadata": {
        "id": "VFf6jdJZW0EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, precision, recall, f1, report = normalize_dataset_and_train_model(new_train_df)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8N5F5C_WzE8",
        "outputId": "962b0f91-2902-4d8e-bb50-644579a56431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data transformation complete\n",
            "Training complete\n",
            "Accuracy: 0.9976744186046511\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1 Score: 0.0\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       429\n",
            "           1       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           1.00       430\n",
            "   macro avg       0.50      0.50      0.50       430\n",
            "weighted avg       1.00      1.00      1.00       430\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}